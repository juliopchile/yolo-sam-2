{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "# Specify the path to the main directory, this is the segment-anything-2 path\n",
    "main_directory = \"/home/asdasd/segment-anything-2\"\n",
    "\n",
    "# Change the current working directory to the main directory\n",
    "os.chdir(main_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# select the device for computation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "elif device.type == \"mps\":\n",
    "    print(\n",
    "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions to show results of SAM2 segmentations, using Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility_functions import show_mask, show_points, show_box, show_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions to show results of SAM2 segmentations, using OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility_functions import draw_masks_on_image, draw_points, draw_boxes, get_bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the YOLO and SAM2 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "yolo_checkpoint = \"/home/asdasd/yolo-sam-2/yolo_weights/Salmons_YOLOv8.pt\"\n",
    "yolo_segmentator = YOLO(model=yolo_checkpoint, task=\"segment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "sam2_checkpoint = \"checkpoints/sam2_hiera_large.pt\" # sam2_hiera_tiny, sam2_hiera_small, sam2_hiera_base_plus, sam2_hiera_large\n",
    "model_cfg = \"sam2_hiera_l.yaml\"                     # sam2_hiera_t, sam2_hiera_s, sam2_hiera_b+, sam2_hiera_l\n",
    "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=\"cuda\")\n",
    "\n",
    "predictor = SAM2ImagePredictor(sam2_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images from video\n",
    "Frames are stored in a list of paths for each video frame, each frame is stored as a JPEG. This is not necesary when using the SAM2ImagePredictor class but is necesary for the SAM2VideoPredictor, so the code is shown here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `video_dir` a directory of JPEG frames with filenames like `<frame_index>.jpg`\n",
    "video_dir = \"/home/asdasd/yolo-sam-2/videos/SHORT_azul_100\"\n",
    "\n",
    "# scan all the JPEG frame names in this directory\n",
    "frame_names = [\n",
    "    p for p in os.listdir(video_dir)\n",
    "    if os.path.splitext(p)[-1] in [\".jpg\", \".jpeg\", \".JPG\", \".JPEG\"]\n",
    "]\n",
    "frame_names.sort(key=lambda p: int(os.path.splitext(p)[0]))\n",
    "frame_paths = [os.path.join(video_dir, frame_name) for frame_name in frame_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now on Video\n",
    "We do inference frame by frame, this is kinda slow since we need to load each image separatedly and inference is done at the frame level. The results quality are already better than the ones obtained with SAM, MobileSAM and FastSAM but we are still not using SAM2 at it's full capability, for such see the video_predictor_with_prompt notebook were the usage of the SAM2VideoPredictor class is tested with prompts in each frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00000.jpg: 384x640 11 salmons, 53.0ms\n",
      "Speed: 1.4ms preprocess, 53.0ms inference, 78.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asdasd/segment-anything-2/sam2/sam2_image_predictor.py:324: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  box = torch.as_tensor(box, dtype=torch.float, device=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00001.jpg: 384x640 11 salmons, 24.1ms\n",
      "Speed: 2.2ms preprocess, 24.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00002.jpg: 384x640 11 salmons, 15.9ms\n",
      "Speed: 1.5ms preprocess, 15.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00003.jpg: 384x640 13 salmons, 16.8ms\n",
      "Speed: 1.9ms preprocess, 16.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00004.jpg: 384x640 13 salmons, 15.4ms\n",
      "Speed: 1.6ms preprocess, 15.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00005.jpg: 384x640 13 salmons, 15.6ms\n",
      "Speed: 1.5ms preprocess, 15.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00006.jpg: 384x640 11 salmons, 33.0ms\n",
      "Speed: 1.7ms preprocess, 33.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00007.jpg: 384x640 11 salmons, 16.9ms\n",
      "Speed: 1.4ms preprocess, 16.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00008.jpg: 384x640 12 salmons, 15.4ms\n",
      "Speed: 1.5ms preprocess, 15.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00009.jpg: 384x640 12 salmons, 39.9ms\n",
      "Speed: 1.8ms preprocess, 39.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00010.jpg: 384x640 15 salmons, 16.6ms\n",
      "Speed: 1.5ms preprocess, 16.6ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00011.jpg: 384x640 16 salmons, 15.5ms\n",
      "Speed: 1.4ms preprocess, 15.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00012.jpg: 384x640 16 salmons, 16.2ms\n",
      "Speed: 1.5ms preprocess, 16.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00013.jpg: 384x640 14 salmons, 16.3ms\n",
      "Speed: 1.4ms preprocess, 16.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00014.jpg: 384x640 15 salmons, 61.6ms\n",
      "Speed: 2.8ms preprocess, 61.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00015.jpg: 384x640 15 salmons, 15.3ms\n",
      "Speed: 1.6ms preprocess, 15.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00016.jpg: 384x640 15 salmons, 57.2ms\n",
      "Speed: 2.2ms preprocess, 57.2ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00017.jpg: 384x640 14 salmons, 16.9ms\n",
      "Speed: 1.3ms preprocess, 16.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00018.jpg: 384x640 15 salmons, 15.3ms\n",
      "Speed: 1.5ms preprocess, 15.3ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00019.jpg: 384x640 12 salmons, 16.8ms\n",
      "Speed: 1.4ms preprocess, 16.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00020.jpg: 384x640 12 salmons, 16.0ms\n",
      "Speed: 1.5ms preprocess, 16.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00021.jpg: 384x640 13 salmons, 15.8ms\n",
      "Speed: 1.4ms preprocess, 15.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00022.jpg: 384x640 14 salmons, 15.7ms\n",
      "Speed: 1.3ms preprocess, 15.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00023.jpg: 384x640 14 salmons, 49.8ms\n",
      "Speed: 1.7ms preprocess, 49.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00024.jpg: 384x640 12 salmons, 15.8ms\n",
      "Speed: 1.4ms preprocess, 15.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00025.jpg: 384x640 12 salmons, 15.6ms\n",
      "Speed: 1.4ms preprocess, 15.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00026.jpg: 384x640 12 salmons, 15.7ms\n",
      "Speed: 1.4ms preprocess, 15.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00027.jpg: 384x640 14 salmons, 15.3ms\n",
      "Speed: 1.4ms preprocess, 15.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00028.jpg: 384x640 14 salmons, 15.2ms\n",
      "Speed: 1.4ms preprocess, 15.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00029.jpg: 384x640 13 salmons, 55.0ms\n",
      "Speed: 1.6ms preprocess, 55.0ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00030.jpg: 384x640 14 salmons, 38.0ms\n",
      "Speed: 1.5ms preprocess, 38.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00031.jpg: 384x640 14 salmons, 14.9ms\n",
      "Speed: 1.4ms preprocess, 14.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00032.jpg: 384x640 13 salmons, 38.1ms\n",
      "Speed: 1.6ms preprocess, 38.1ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00033.jpg: 384x640 13 salmons, 15.2ms\n",
      "Speed: 1.6ms preprocess, 15.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00034.jpg: 384x640 15 salmons, 55.3ms\n",
      "Speed: 1.8ms preprocess, 55.3ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00035.jpg: 384x640 11 salmons, 15.0ms\n",
      "Speed: 1.4ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00036.jpg: 384x640 11 salmons, 15.2ms\n",
      "Speed: 1.4ms preprocess, 15.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00037.jpg: 384x640 11 salmons, 15.4ms\n",
      "Speed: 1.5ms preprocess, 15.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00038.jpg: 384x640 9 salmons, 14.8ms\n",
      "Speed: 1.4ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00039.jpg: 384x640 8 salmons, 16.3ms\n",
      "Speed: 1.4ms preprocess, 16.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00040.jpg: 384x640 10 salmons, 17.7ms\n",
      "Speed: 1.5ms preprocess, 17.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00041.jpg: 384x640 9 salmons, 15.9ms\n",
      "Speed: 1.4ms preprocess, 15.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00042.jpg: 384x640 8 salmons, 16.2ms\n",
      "Speed: 1.4ms preprocess, 16.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00043.jpg: 384x640 11 salmons, 55.4ms\n",
      "Speed: 1.5ms preprocess, 55.4ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00044.jpg: 384x640 11 salmons, 16.5ms\n",
      "Speed: 1.4ms preprocess, 16.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00045.jpg: 384x640 10 salmons, 57.2ms\n",
      "Speed: 1.5ms preprocess, 57.2ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00046.jpg: 384x640 11 salmons, 17.1ms\n",
      "Speed: 1.5ms preprocess, 17.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00047.jpg: 384x640 11 salmons, 16.2ms\n",
      "Speed: 1.6ms preprocess, 16.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00048.jpg: 384x640 14 salmons, 15.9ms\n",
      "Speed: 1.5ms preprocess, 15.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00049.jpg: 384x640 14 salmons, 15.6ms\n",
      "Speed: 1.4ms preprocess, 15.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00050.jpg: 384x640 12 salmons, 15.2ms\n",
      "Speed: 1.3ms preprocess, 15.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00051.jpg: 384x640 12 salmons, 23.1ms\n",
      "Speed: 1.5ms preprocess, 23.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/asdasd/yolo-sam-2/videos/SHORT_azul_100/00052.jpg: 384x640 12 salmons, 16.7ms\n",
      "Speed: 1.4ms preprocess, 16.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "# Video segmentation using bbox as prompt, frame by frame.\n",
    "mask_input = None\n",
    "\n",
    "for frame in frame_paths:\n",
    "    # Load the image using OpenCV\n",
    "    image = cv2.imread(frame)\n",
    "\n",
    "    # Convert the image to RGB (OpenCV loads images in BGR by default)\n",
    "    image_RGB = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Set the SAM2 predictor to the image\n",
    "    predictor.set_image(image_RGB)\n",
    "    \n",
    "    # Get the bboxes with YOLO\n",
    "    results = yolo_segmentator.predict(frame)\n",
    "    \n",
    "    input_boxes = get_bboxes(results)\n",
    "    \n",
    "    # Do inference with SAM2\n",
    "    masks, scores, logits = predictor.predict(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        box=input_boxes,  # Using bboxes as propmt\n",
    "        mask_input=mask_input,\n",
    "        multimask_output=False,\n",
    "    )\n",
    "    mask_input = None\n",
    "\n",
    "    # Draw the boxes and mask on the image\n",
    "    image_with_boxes = draw_boxes(image, input_boxes)\n",
    "    image_with_masks = draw_masks_on_image(image_with_boxes, masks, random_color=True, borders=True)\n",
    "    \n",
    "    # The original YOLO result\n",
    "    yolo_image = results[0].plot(color_mode=\"instance\")\n",
    "\n",
    "    # Show the image with masks\n",
    "    cv2.imshow('Image with Maskss', cv2.resize(image_with_masks, (image_with_masks.shape[1] // 2, image_with_masks.shape[0] // 2)))\n",
    "    cv2.imshow(\"Yolo results\", cv2.resize(yolo_image, (image_with_masks.shape[1] // 2, image_with_masks.shape[0] // 2)))\n",
    "\n",
    "    # press 'q' with the output window focused to exit.\n",
    "    # waits 1 ms every loop to process key presses\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo-sam-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
